{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MITBIHProcessor:\n",
    "    def __init__(self, data_path, output_path):\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.records = None\n",
    "        self.annotations = None\n",
    "        self.sampling_rate = 360  \n",
    "\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "            os.makedirs(os.path.join(output_path, 'features'))\n",
    "            os.makedirs(os.path.join(output_path, 'labels'))\n",
    "    \n",
    "    def load_record_list(self):\n",
    "        self.records = [\n",
    "            '100', '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
    "            '111', '112', '113', '114', '115', '116', '117', '118', '119', '121',\n",
    "            '122', '123', '124', '200', '201', '202', '203', '205', '207', '208',\n",
    "            '209', '210', '212', '213', '214', '215', '217', '219', '220', '221',\n",
    "            '222', '223', '228', '230', '231', '232', '233', '234'\n",
    "        ]\n",
    "        return self.records\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        annotations_info = []\n",
    "        \n",
    "        for record in self.records:\n",
    "            try:\n",
    "                annotation = wfdb.rdann(os.path.join(self.data_path, record), 'atr')\n",
    "                \n",
    "                unique_symbols = set(annotation.symbol)\n",
    "                beat_counts = {}\n",
    "                \n",
    "                for symbol in unique_symbols:\n",
    "                    if symbol not in ['', '~', '|', ']', '[', 'p', 't', 'u', '`', '^', 'Q']:\n",
    "                        count = np.sum(np.array(annotation.symbol) == symbol)\n",
    "                        beat_counts[symbol] = count\n",
    "                \n",
    "                annotations_info.append({\n",
    "                    'record': record,\n",
    "                    'beat_types': beat_counts,\n",
    "                    'total_beats': len(annotation.symbol),\n",
    "                    'sampling_rate': annotation.fs,\n",
    "                    'duration_minutes': len(annotation.symbol) / (annotation.fs * 60) * 30  # 估计时长\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading annotation for record {record}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.annotations = pd.DataFrame(annotations_info)\n",
    "        return self.annotations\n",
    "    \n",
    "    def map_beat_to_diagnosis(self, beat_symbol):\n",
    "\n",
    "        if beat_symbol in ['N', 'L', 'R', 'e', 'j']:\n",
    "            return 'NORM'  \n",
    "        elif beat_symbol in ['A', 'a', 'J', 'S']:\n",
    "            return 'SVPB'  \n",
    "        elif beat_symbol in ['V', 'E']:\n",
    "            return 'VPC'\n",
    "        elif beat_symbol in ['F']:\n",
    "            return 'FUSION'\n",
    "        elif beat_symbol in ['/', 'f', 'Q']:\n",
    "            return 'UNKNOWN'  \n",
    "        else:\n",
    "            return 'UNKNOWN'\n",
    "    \n",
    "    def resample_signal(self, signal, original_rate, target_rate=250):\n",
    "        duration = len(signal) / original_rate\n",
    "        num_samples_target = int(duration * target_rate)\n",
    "        \n",
    "        t_original = np.linspace(0, duration, len(signal))\n",
    "        t_target = np.linspace(0, duration, num_samples_target)\n",
    "        \n",
    "        f = interpolate.interp1d(t_original, signal, kind='linear')\n",
    "        resampled_signal = f(t_target)\n",
    "        \n",
    "        return resampled_signal\n",
    "    \n",
    "    def normalize_signal(self, signal):\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        normalized = scaler.fit_transform(signal.reshape(-1, 1)).flatten()\n",
    "        return normalized\n",
    "    \n",
    "    def extract_heartbeat_segments(self, record_name, segment_length=250, lead=0):\n",
    "\n",
    "        try:\n",
    "            # 读取记录和注释\n",
    "            record_path = os.path.join(self.data_path, record_name)\n",
    "            signals, fields = wfdb.rdsamp(record_path)\n",
    "            annotation = wfdb.rdann(record_path, 'atr')\n",
    "\n",
    "            if lead >= signals.shape[1]:\n",
    "                lead = 0 \n",
    "                \n",
    "            ecg_signal = signals[:, lead]\n",
    "\n",
    "            if fields['fs'] != 250:\n",
    "                ecg_signal = self.resample_signal(ecg_signal, fields['fs'], 250)\n",
    "            \n",
    "            ecg_signal = self.normalize_signal(ecg_signal)\n",
    "\n",
    "            segments = []\n",
    "            labels = []\n",
    "            valid_beats = 0\n",
    "            \n",
    "            for i, sample in enumerate(annotation.sample):\n",
    "                if sample >= segment_length//2 and sample + segment_length//2 <= len(ecg_signal):\n",
    "                    start_idx = sample - segment_length//2\n",
    "                    end_idx = sample + segment_length//2\n",
    "                    \n",
    "                    segment = ecg_signal[start_idx:end_idx]\n",
    "                    \n",
    "                    if len(segment) == segment_length:\n",
    "                        segments.append(segment)\n",
    "                        beat_label = self.map_beat_to_diagnosis(annotation.symbol[i])\n",
    "                        labels.append(beat_label)\n",
    "                        valid_beats += 1\n",
    "            \n",
    "            segments_array = np.array(segments)\n",
    "            labels_array = np.array(labels)\n",
    "            \n",
    "            print(f\"Record {record_name}: Extracted {valid_beats} valid heartbeat segments\")\n",
    "            \n",
    "            return segments_array, labels_array\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {record_name}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def create_patient_wise_dataset(self, beats_per_patient=1000):\n",
    "\n",
    "        print(\"Creating patient-wise dataset...\")\n",
    "        \n",
    "        patient_data = {}\n",
    "        patient_labels = {}\n",
    "        \n",
    "        for record in self.records:\n",
    "            print(f\"Processing record: {record}\")\n",
    "\n",
    "            segments, labels = self.extract_heartbeat_segments(record, lead=0)\n",
    "            \n",
    "            if segments is not None and len(segments) > 0:\n",
    "\n",
    "                patient_id = f\"MIT_{record}\"\n",
    "\n",
    "                if len(segments) > beats_per_patient:\n",
    "                    indices = np.random.choice(len(segments), beats_per_patient, replace=False)\n",
    "                    segments = segments[indices]\n",
    "                    labels = labels[indices]\n",
    "                elif len(segments) < beats_per_patient:\n",
    "                    repeat_times = beats_per_patient // len(segments)\n",
    "                    remainder = beats_per_patient % len(segments)\n",
    "                    \n",
    "                    segments_repeated = np.tile(segments, (repeat_times, 1))\n",
    "                    labels_repeated = np.tile(labels, repeat_times)\n",
    "                    \n",
    "                    if remainder > 0:\n",
    "                        indices = np.random.choice(len(segments), remainder, replace=False)\n",
    "                        segments = np.vstack([segments_repeated, segments[indices]])\n",
    "                        labels = np.concatenate([labels_repeated, labels[indices]])\n",
    "                    else:\n",
    "                        segments = segments_repeated\n",
    "                        labels = labels_repeated\n",
    "                \n",
    "                segments = segments.reshape(-1, segments.shape[-1], 1)\n",
    "                \n",
    "                patient_data[patient_id] = segments\n",
    "                patient_labels[patient_id] = labels\n",
    "                \n",
    "                np.save(os.path.join(self.output_path, 'features', f'feature_{patient_id}.npy'), segments)\n",
    "        \n",
    "        self.create_label_file(patient_labels)\n",
    "        \n",
    "        return patient_data, patient_labels\n",
    "    \n",
    "    def create_label_file(self, patient_labels):\n",
    "\n",
    "        label_data = []\n",
    "        \n",
    "        for patient_id, labels in patient_labels.items():\n",
    "\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            primary_diagnosis = unique[np.argmax(counts)]\n",
    "\n",
    "            diagnosis_map = {'NORM': 0, 'SVPB': 1, 'VPC': 2, 'FUSION': 3, 'UNKNOWN': 4}\n",
    "            label_num = diagnosis_map.get(primary_diagnosis, 4)\n",
    "\n",
    "            record_num = int(patient_id.split('_')[1])\n",
    "            \n",
    "            label_data.append([label_num, record_num])\n",
    "        \n",
    "        label_array = np.array(label_data)\n",
    "        np.save(os.path.join(self.output_path, 'labels', 'label.npy'), label_array)\n",
    "        \n",
    "        label_df = pd.DataFrame(label_array, columns=['label', 'patient_id'])\n",
    "        diagnosis_map_reverse = {v: k for k, v in diagnosis_map.items()}\n",
    "        label_df['diagnosis'] = label_df['label'].map(diagnosis_map_reverse)\n",
    "        label_df.to_csv(os.path.join(self.output_path, 'labels', 'label_info.csv'), index=False)\n",
    "        \n",
    "        print(f\"Label distribution:\\n{label_df['diagnosis'].value_counts()}\")\n",
    "    \n",
    "    def create_10s_segments_dataset(self, segment_length=2500):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            segment_length: 10秒片段的长度（250Hz * 10s = 2500个点）\n",
    "        \"\"\"\n",
    "        print(\"Creating 10-second segments dataset...\")\n",
    "        \n",
    "        segment_data = {}\n",
    "        segment_labels = {}\n",
    "        segment_id = 0\n",
    "        \n",
    "        for record in self.records:\n",
    "            print(f\"Processing record: {record}\")\n",
    "            \n",
    "            try:\n",
    "                record_path = os.path.join(self.data_path, record)\n",
    "                signals, fields = wfdb.rdsamp(record_path)\n",
    "                \n",
    "                if signals.shape[1] >= 1:\n",
    "                    ecg_signal = signals[:, 0]\n",
    "                    \n",
    "                    if fields['fs'] != 250:\n",
    "                        ecg_signal = self.resample_signal(ecg_signal, fields['fs'], 250)\n",
    "                    ecg_signal = self.normalize_signal(ecg_signal)\n",
    "                    \n",
    "                    num_segments = len(ecg_signal) // segment_length\n",
    "                    \n",
    "                    for i in range(num_segments):\n",
    "                        start_idx = i * segment_length\n",
    "                        end_idx = (i + 1) * segment_length\n",
    "                        \n",
    "                        if end_idx <= len(ecg_signal):\n",
    "                            segment = ecg_signal[start_idx:end_idx]\n",
    "                            segment_id_str = f\"MIT_{record}_seg_{i}\"\n",
    "                            \n",
    "                            segment_data[segment_id_str] = segment.reshape(1, -1, 1)\n",
    "                            segment_labels[segment_id_str] = f\"MIT_{record}\"\n",
    "                            \n",
    "                            np.save(os.path.join(self.output_path, 'features', f'feature_{segment_id_str}.npy'), \n",
    "                                   segment.reshape(1, -1, 1))\n",
    "                            \n",
    "                            segment_id += 1\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record {record} for 10s segments: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Created {segment_id} 10-second segments\")\n",
    "        return segment_data, segment_labels\n",
    "    \n",
    "    def generate_dataset_summary(self):\n",
    "        \n",
    "        summary = {\n",
    "            'total_records': len(self.records),\n",
    "            'sampling_rate': 250,  \n",
    "            'segment_length': 250, \n",
    "            'leads_used': 1,  # 使用MLII导联\n",
    "            'diagnosis_categories': ['NORM', 'SVPB', 'VPC', 'FUSION', 'UNKNOWN']\n",
    "        }\n",
    "        \n",
    "    \n",
    "        summary_df = pd.DataFrame([summary])\n",
    "        summary_df.to_csv(os.path.join(self.output_path, 'dataset_summary.csv'), index=False)\n",
    "        \n",
    "        return summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2d0bb5cf4c801da"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    mit_bih_path = r'C:\\Users\\28199\\Desktop\\mit-bih-arrhythmia-database-1.0.0' \n",
    "    output_path = r'C:\\Users\\28199\\Desktop\\mit_bih_processed'  \n",
    "\n",
    "    processor = MITBIHProcessor(mit_bih_path, output_path)\n",
    "\n",
    "    records = processor.load_record_list()\n",
    "    print(f\"Loaded {len(records)} MIT-BIH records\")\n",
    "\n",
    "    annotations = processor.load_annotations()\n",
    "    print(\"Annotations summary:\")\n",
    "    print(annotations.head())\n",
    "\n",
    "    patient_data, patient_labels = processor.create_patient_wise_dataset(beats_per_patient=1000)\n",
    "    \n",
    "    # segment_data, segment_labels = processor.create_10s_segments_dataset()\n",
    "    \n",
    "    summary = processor.generate_dataset_summary()\n",
    "    print(\"Dataset summary:\")\n",
    "    print(summary)\n",
    "    \n",
    "    print(f\"Processing completed! Data saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbc247edea73165"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4a6f5aba02cc104d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
